### 6.1 结构定义

* 文件位置：models-yolov5.yaml

* 参数介绍：

  * ```python
    # Parameters
    nc: 80  # number of classes 分类类别
    depth_multiple: 0.33  # model depth multiple 深度缩放系数，影响Backbone中的number重复测数，与其相乘，与1对比去最大值
    width_multiple: 0.50  # layer channel multiple 广度缩放系数，影响输出通道数（args中第一个元素）
    anchors: # 锚框（锚定框）：三个特征图，每个特征图上有三组锚框
      - [10,13, 16,30, 33,23]  # P3/8
      - [30,61, 62,45, 59,119]  # P4/16
      - [116,90, 156,198, 373,326]  # P5/32
    ```

  * ![在这里插入图片描述](https://img-blog.csdnimg.cn/3862caa95dd343f090808690e2648e23.png)

  * Backbone：

    * ```python
      # YOLOv5 v6.0 backbone
      backbone:
        # [from, number, module, args] [从哪来（-1就是上一层），当前模块重复几次，具体用的哪个模块，实例化模块需要存入的参数]
        [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2 网络编号-卷积编号/原始图像缩放倍数
         [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
         [-1, 3, C3, [128]],
         [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
         [-1, 6, C3, [256]],
         [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
         [-1, 9, C3, [512]],
         [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
         [-1, 3, C3, [1024]],
         [-1, 1, SPPF, [1024, 5]],  # 9
        ]
      ```

  * head：

    * ```python
      # YOLOv5 v6.0 head
      head:
        [[-1, 1, Conv, [512, 1, 1]],
         [-1, 1, nn.Upsample, [None, 2, 'nearest']],
         [[-1, 6], 1, Concat, [1]],  # cat backbone P4
         [-1, 3, C3, [512, False]],  # 13
      
         [-1, 1, Conv, [256, 1, 1]],
         [-1, 1, nn.Upsample, [None, 2, 'nearest']],
         [[-1, 4], 1, Concat, [1]],  # cat backbone P3
         [-1, 3, C3, [256, False]],  # 17 (P3/8-small) 8倍下采样用于检测小型物体
      
         [-1, 1, Conv, [256, 3, 2]],
         [[-1, 14], 1, Concat, [1]],  # cat head P4
         [-1, 3, C3, [512, False]],  # 20 (P4/16-medium) 16倍下采样用于检测中等物体
      
         [-1, 1, Conv, [512, 3, 2]],
         [[-1, 10], 1, Concat, [1]],  # cat head P5
         [-1, 3, C3, [1024, False]],  # 23 (P5/32-large) 32倍下采用用于检测大型物体
      
         [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
        ]
      ```

### 6.2 修改网络结构

#### 6.1.2 加入C2f

（1）下载yolo v8：

* [Releases · ultralytics/ultralytics (github.com)](https://github.com/ultralytics/ultralytics)
* C2f所在文件位置：
  * "E:\AI\Package\ultralytics-main\ultralytics-main\ultralytics\nn\modules\block.py"

（2）加入新增网络结构：

* 目标位置：models/commons.py
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/2ae8070a-7051-42fc-ae85-eee83ae6e16f)

* C2f中的Bottleneck多了k参数，因此需要添加
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/dc1e1567-d1ed-47d2-9b99-c91e2502ba9d)

* 加入C2f前缀
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/f1f13d24-b605-4093-a65e-6485e48fa7f6)


（3）设定网络结构的传参细节

* 目标位置：models/yolo.py
* 在yolo.py中加入C2f的接口，以为C2f和C3传参一致，所以可以在parse_model代码中有C3的地方加入即可：
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/47f71fb3-35c3-4eb6-91e0-769a8eb397ed)


（4）修改现有模型结构配置文件

* 目标位置：models/yolov5*.yaml
* 复制文件：
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/b5d16adb-3103-43a1-b780-b005865562e3)

* 将backbone中的C3改为C2f
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/77b431a1-df33-4929-9307-0cf43f7a1b77)


（5）训练时指定模型结构配置文件

* 目标位置：train.py

* 修改cfg参数：

  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/47cb6425-0ffa-4638-8dcf-77565bb6dd40)


  * ```python
    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')
    #加入自己建立的yaml文件：
    parser.add_argument('--cfg', type=str, default=ROOT / 'models/yolov5s-c2f.yaml', help='model.yaml path')
    ```

* 训练：

  * (yolov5) E:\AI\Package\yolov5-7.0>python train.py
    * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/3f0fc353-c477-4d0a-b04b-91a8c3c6ba20)

    * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/d7041d5b-59ec-4862-b996-f81b5d448efe)


#### 6.2.2 引入SE注意力机制

（1）借鉴代码：

* [ZhugeKongan/Attention-mechanism-implementation: Self-attention、Non-local、SE、SK、CBAM、DANet (github.com)](https://github.com/ZhugeKongan/Attention-mechanism-implementation)

（2）文件位置：

* "E:\AI\Package\Attention-mechanism-implementation-main\models\SE_block.py"

（3）加入新的网络结构：

* ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/81bb9285-7425-46d7-abac-2990d60176bb)

* ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/98386f50-e6c7-4c17-a3ff-75ce12dc18f9)

（4）修改现有的模型配置文件：

* ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/eaee1593-6192-4b3c-aa4c-96de4716b2c4)


* 📌**<u>引入形层需要注意编号</u>**，因为在Backbone中多加了一层SE，因此此时SE是第10层，那么后面Head中大于10层的都需要加1层：

  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/c98c5e78-8fa1-472b-9305-08624c4c1e0c)


  * ```python
    # YOLOv5 🚀 by Ultralytics, GPL-3.0 license
    
    # Parameters
    nc: 80  # number of classes
    depth_multiple: 0.33  # model depth multiple
    width_multiple: 0.50  # layer channel multiple
    anchors:
      - [10,13, 16,30, 33,23]  # P3/8
      - [30,61, 62,45, 59,119]  # P4/16
      - [116,90, 156,198, 373,326]  # P5/32
    
    # YOLOv5 v6.0 backbone
    backbone:
      # [from, number, module, args]
      [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
       [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
       [-1, 3, C3, [128]],
       [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
       [-1, 6, C3, [256]],
       [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
       [-1, 9, C3, [512]],
       [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
       [-1, 3, C3, [1024]],
       [-1, 1, SPPF, [1024, 5]],  # 9
       [-1, 1, SE, [1024, 2]] #10
      ]
    
    # YOLOv5 v6.0 head
    head:
      [[-1, 1, Conv, [512, 1, 1]],
       [-1, 1, nn.Upsample, [None, 2, 'nearest']],
       [[-1, 6], 1, Concat, [1]],  # cat backbone P4
       [-1, 3, C3, [512, False]],  # 13-14
    
       [-1, 1, Conv, [256, 1, 1]],
       [-1, 1, nn.Upsample, [None, 2, 'nearest']],
       [[-1, 4], 1, Concat, [1]],  # cat backbone P3
       [-1, 3, C3, [256, False]],  # 17-18 (P3/8-small)
    
       [-1, 1, Conv, [256, 3, 2]],
       [[-1, 15], 1, Concat, [1]],  # cat head P4
       [-1, 3, C3, [512, False]],  # 20-21 (P4/16-medium)
    
       [-1, 1, Conv, [512, 3, 2]],
       [[-1, 11], 1, Concat, [1]],  # cat head P5
       [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)
    
       [[18, 21, 24], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
      ]
    ```

（5）设定网络结构的传参细节：

* **当新的自定义模块中存在输入输出维度时，要使用gw调整输出维度！**

* 在yolo.py中的parse_modle()函数中加入对SE的处理：

  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/1d6cb241-a466-4330-93f8-9f8faff71c12)


  * ```python
    elif m is SE:
        c1 = ch[f]  #c1等于上一层
        c2 = args[0] #c1和c2一样
        if c2 != no:  # if not output
            c2 = make_divisible(c2 * gw, 8)
        args = [c1, *args[1:]]
    ```

（6）训练时指定模型结构配置文件

* 目标位置：train.py

* 修改cfg参数：

  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/6c422abf-85b0-432c-9f98-10bf8c72aaf5)


  * ```python
    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')
    #加入自己建立的yaml文件：
    parser.add_argument('--cfg', type=str, default=ROOT / 'models/yolov5s-se.yaml', help='model.yaml path')
    ```

* 训练：

  * (yolov5) E:\AI\Package\yolov5-7.0>python train.py
    * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/0e549a42-3f5f-4c47-8f77-fc7f0e348901)

    * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/ba97f865-5a7e-4988-a153-08384761db46)
   
#### 6.2.3 替换主干网络MobileNet

（1）借鉴代码：

* torchvision

（2）文件位置：

* 新建文件【demo.ipynb】，加载mobilenet网络：
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/1d8a493a-8517-4f3c-96b0-58bc4163efb1)

* 安装torchinfo：
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/83e3f097-f946-4df0-ada8-e69a6b249b56)


* 导入torchinfo中的summary工具查看模型结构：
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/74ea0b0b-4ab2-4221-a21e-3dfc81579ec6)

  * 主要用的是Sequential模块：
    * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/d86e00be-f199-478e-b054-776cf145b762)

* 打开features特征提取模块：
  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/19344bd7-5398-4ab9-90a5-2a4f1ec8fedd)

  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/42b580ff-96bd-4b61-b281-bd9325ce0b11)



（3）加入新的网络结构：

* 首先我们要分析模型输出的小、中、大三个尺寸网络位置：

  * 8倍下采样：
    * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/ef279a9d-de68-4fd6-8e52-c77389480813)

  * 16倍下采样：
    * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/d237b8b7-8d4c-4159-9680-59fb4fd65ffe)

  * 32倍下采样：
    * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/5731a11c-8816-4c1d-bdaf-93752dc8adf4)


* 使用feature指令查看模型三个部分的网络结构：

  * ```python
    model.features[:4]
    model.features[4:9]
    model.features[9:]
    ```

  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/300ddb62-5e93-481f-9d2d-291f10588ab2)


* 在【common.py】中加入MobileNet类的定义：

  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/af00fa0c-7fe4-4e40-bb75-6da251a2a771)


  * ```python
    import torchvision.models as models
    
    
    class MobileNetV3(nn.Module):
        def __init__(self, slice):
            super(MobileNetV3, self).__init__()
            self.model = None
            if slice == 1:
                self.model = models.mobilenet_v3_small(pretrained=True).features[:4]
            elif slice == 2:
                self.model = models.mobilenet_v3_small(pretrained=True).features[4:9]
            else:
                self.model = models.mobilenet_v3_small(pretrained=True).features[9:] 
    
        def forward(self, x):
            return self.model
    ```

* 在【yolov5s-mobilenet.yaml】中修改模型结构：

  * ```python
    # YOLOv5 🚀 by Ultralytics, GPL-3.0 license
    
    # Parameters
    nc: 80  # number of classes
    depth_multiple: 0.33  # model depth multiple
    width_multiple: 0.50  # layer channel multiple
    anchors:
      - [10,13, 16,30, 33,23]  # P3/8
      - [30,61, 62,45, 59,119]  # P4/16
      - [116,90, 156,198, 373,326]  # P5/32
    
    # YOLOv5 v6.0 backbone
    backbone:
      # [from, number, module, args]
      [
      #  [-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
      #  [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
      #  [-1, 3, C3, [128]],
      #  [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
      #  [-1, 6, C3, [256]],
      #  [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
      #  [-1, 9, C3, [512]],
      #  [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
      #  [-1, 3, C3, [1024]],
      #  [-1, 1, SPPF, [1024, 5]],  # 9
       [-1, 1, MobileNetV3, [24, 1]],     # 0-P3/8  80*80  # [24, 1]表示[输出通道数， 切片标号]
       [-1, 1, MobileNetV3, [48, 2]],     # 1-P4/16  40*40
       [-1, 1, MobileNetV3, [576, 3]]     # 2-P5/32  20*20
      ]
    
    # YOLOv5 v6.0 head
    head:
      [[-1, 1, Conv, [512, 1, 1]],  # 3-10
       [-1, 1, nn.Upsample, [None, 2, 'nearest']],  # 4-11
       [[-1, 1], 1, Concat, [1]],  # 5-12 cat backbone P4(#1)     # 40 * 40 需要和Backbone中的相同尺寸网络层拼接
       [-1, 3, C3, [512, False]],  # 6-13
    
       [-1, 1, Conv, [256, 1, 1]], # 7-14
       [-1, 1, nn.Upsample, [None, 2, 'nearest']], # 8-15
       [[-1, 0], 1, Concat, [1]],  # 9-16 cat backbone P3(#0)     # 80 * 80
       [-1, 3, C3, [256, False]],  # 10-17 (P3/8-small)
    
       [-1, 1, Conv, [256, 3, 2]], # 11-18
       [[-1, 7], 1, Concat, [1]],  # 12-19 cat head P4(#7)        # 40 * 40
       [-1, 3, C3, [512, False]],  # 13-20 (P4/16-medium)
    
       [-1, 1, Conv, [512, 3, 2]], # 14-21
       [[-1, 3], 1, Concat, [1]],  # 15-22 cat head P5(#3)        # 20 * 20
       [-1, 3, C3, [1024, False]],  # 16-23 (P5/32-large)
    
       [[10, 13, 16], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
      ]
    ```

  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/03a77128-e01d-41d5-b30f-8473a11aaade)


* 在【yolo.py】中进行注册：

  * ```python
    elif m is MobileNetV3:
        c2 = args[0]
        args = args[1:]
    ```

  * ![image](https://github.com/CoderSuHang/TensorRT-Learning-Note/assets/104765251/fe6be7ea-bc6f-4ea2-bad0-efa5710e68bb)


* 【训练】运行【train.py】：

  * 原始训练结果：
    * ![image-20240318175102418](C:\Users\10482\AppData\Roaming\Typora\typora-user-images\image-20240318175102418.png)
  * 改进后的训练结果：
    * 报错：
      * TypeError: conv2d(): argument 'input' (position 1) must be Tensor, not Sequential
        * ![image-20240318180904020](C:\Users\10482\AppData\Roaming\Typora\typora-user-images\image-20240318180904020.png)
      * 原因：在【common.py】文件中返回的参数应该是self.model(x)，而不是self.model：
        * ![image-20240318180834131](C:\Users\10482\AppData\Roaming\Typora\typora-user-images\image-20240318180834131.png)
    * 成功结果：
      * ![image-20240318181618138](C:\Users\10482\AppData\Roaming\Typora\typora-user-images\image-20240318181618138.png)
      * 层数变多，但是参数量相较于之前的下降近50%。







